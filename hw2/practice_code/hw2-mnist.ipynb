{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "from datetime import datetime\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import torchvision.models as models\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(dataset, train_ratio, val_ratio, batch_size):\n",
    "    train_dataset = dataset\n",
    "    val_dataset = dataset\n",
    "    test_dataset = dataset\n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(test_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    print(\"--------- INDEX checking ---------\")\n",
    "    print(f\"Original: {indices[:5]}\")\n",
    "    random.shuffle(indices)\n",
    "    print(f\"Shuffled: {indices[:5]}\")\n",
    "    print(\"--------- INDEX shuffled ---------\\n\")\n",
    "\n",
    "    split_train = int(np.floor(train_ratio * num_train))\n",
    "    split_val = split_train + int(np.floor(val_ratio * (num_train-split_train)))\n",
    "    train_idx, val_idx, test_idx = indices[0:split_train], indices[split_train:split_val], indices[split_val:]\n",
    "    merge_dataset = Subset(train_dataset, train_idx)\n",
    "\n",
    "    train_loader = DataLoader(merge_dataset, batch_size=batch_size)\n",
    "    val_loader = DataLoader(Subset(val_dataset, val_idx), batch_size=batch_size)\n",
    "    test_loader = DataLoader(Subset(test_dataset, test_idx), batch_size=batch_size)\n",
    "    \n",
    "    # check dataset\n",
    "    print(f\"Total number of samples: {num_train} datapoints\")\n",
    "    print(f\"Number of train samples: {len(train_loader)} batches/ {len(train_loader.dataset)} datapoints\")\n",
    "    print(f\"Number of val samples: {len(val_loader)} batches/ {len(val_loader.dataset)} datapoints\")\n",
    "    print(f\"Number of test samples: {len(test_loader)} batches/ {len(test_loader.dataset)} datapoints\")\n",
    "    print(f\"\")\n",
    "    \n",
    "    dataloaders = {\n",
    "        \"train\": train_loader,\n",
    "        \"val\": val_loader,\n",
    "        \"test\": test_loader,\n",
    "    }\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_original_data_pytorch(path, train_ratio, val_ratio, batchsize):\n",
    "    # Define a transform to normalize the data\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,), (0.5,))])\n",
    "    \n",
    "    # Download and load the training data\n",
    "    trainset = datasets.MNIST(path, download=True, train=True, transform=transform)\n",
    "    # train_loader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True)\n",
    "    dataloaders = get_dataloaders(trainset,train_ratio, val_ratio, batchsize)\n",
    "    # Download and load the test data\n",
    "    testset = datasets.MNIST(path, download=True, train=False, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batchsize, shuffle=True)\n",
    "\n",
    "    loaders = {\n",
    "    'train': dataloaders['train'],\n",
    "    'val': dataloaders['val'],\n",
    "    'test': test_loader,\n",
    "    }\n",
    "    return loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START LAB\n"
     ]
    }
   ],
   "source": [
    "def pprint(output = '\\n', show_time = False): # print and fprint at the same time\n",
    "    filename = \"hw2-1-MAR27.txt\"\n",
    "    print(output)\n",
    "    with open(filename, 'a') as f:\n",
    "        if show_time:\n",
    "            f.write(datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S] \"))\n",
    "\n",
    "        f.write(str(output))\n",
    "        f.write('\\n')\n",
    "pprint(\"START LAB\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_num = 0\n",
    "\n",
    "    for parameter in model.parameters():\n",
    "        if parameter.requires_grad:\n",
    "            total_num += parameter.numel() \n",
    "    return total_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, model_name, loaders, optim_op, lr, save_model=False):\n",
    "    model = model()\n",
    "    loaders = loaders()\n",
    "    pprint(f\"test {model_name}\", True)\n",
    "    model_parameters_amount = count_parameters(model)\n",
    "    pprint(f\"model total parameters: {model_parameters_amount:,}\")\n",
    "\n",
    "    model = model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if optim_op == 0:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim_op == 1:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    pprint(f\"learning rate={lr}\")\n",
    "    iteration = 0\n",
    "    epochs = 1\n",
    "    start = time.time()\n",
    "    phases = ['train']\n",
    "    for epoch in range(epochs):\n",
    "        for phase in phases:\n",
    "            running_loss = 0.0\n",
    "            correct_predictions = 0\n",
    "            correct_top3_predictions = 0\n",
    "            total_samples = 0\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            for images, labels in tqdm(loaders[phase]): # Iterate over data.\n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    if phase == 'train': # backward + optimize only if in training phase\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Convert outputs to predicted class by selecting the class with the highest score\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                # Accumulate the number of correct predictions\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                \n",
    "                _, top3_preds = outputs.topk(3, 1, True, True)\n",
    "                correct_top3_predictions += sum([labels[i] in top3_preds[i] for i in range(labels.size(0))])\n",
    "\n",
    "                total_samples += labels.size(0)\n",
    "                iteration += 1\n",
    "                # if iteration % 20 == 0:\n",
    "                #     print(iteration)\n",
    "            avg_loss = running_loss / total_samples\n",
    "            top1_accuracy = correct_predictions / total_samples * 100\n",
    "            top3_accuracy = correct_top3_predictions / total_samples * 100\n",
    "            pprint(f\"Epoch [{epoch+1}/{epochs}], phase: {phase}, samples: {total_samples}, Loss: {avg_loss:.4f}, Top-1 Accuracy: {top1_accuracy:.2f}%, Top-3 Accuracy: {top3_accuracy:.2f}%\")\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    pprint(f\"Elapsed time: {duration} seconds\")\n",
    "    if save_model:\n",
    "        model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "        model_scripted.save(f'{model_name}.pt') # Save\n",
    "        pprint(f\"weight saved as: {model_name}.pt\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, act_layer):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "        if act_layer == \"softmax\":\n",
    "            self.activation = F.softmax\n",
    "        elif act_layer == \"sigmoid\":\n",
    "            self.activation = F.sigmoid\n",
    "        elif act_layer == \"ReLU\":\n",
    "            self.activation = F.relu\n",
    "        elif act_layer == \"leakyReLU\":\n",
    "            self.activation = F.leaky_relu\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the input tensor\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 64)  # The image size is reduced to 7x7 after pooling layers\n",
    "        self.fc2 = nn.Linear(64, 10)  # 10 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Convolution -> ReLU -> Pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Convolution -> ReLU -> Pooling\n",
    "        x = torch.flatten(x, 1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))  # Dense layer -> ReLU\n",
    "        x = self.fc2(x)  # Output layer\n",
    "        return F.log_softmax(x, dim=1)  # Log Softmax activation for the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- INDEX checking ---------\n",
      "Original: [0, 1, 2, 3, 4]\n",
      "Shuffled: [20518, 52753, 3741, 48838, 51465]\n",
      "--------- INDEX shuffled ---------\n",
      "\n",
      "Total number of samples: 60000 datapoints\n",
      "Number of train samples: 750 batches/ 48000 datapoints\n",
      "Number of val samples: 94 batches/ 6000 datapoints\n",
      "Number of test samples: 94 batches/ 6000 datapoints\n",
      "\n",
      "test base\n",
      "model total parameters: 50,890\n",
      "learning rate=0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/750 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:10<00:00, 68.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], phase: train, samples: 48000, Loss: 0.0060, Top-1 Accuracy: 88.42%, Top-3 Accuracy: 97.18%\n",
      "Elapsed time: 10.953639507293701 seconds\n",
      "weight saved as: base.pt\n",
      "--------- INDEX checking ---------\n",
      "Original: [0, 1, 2, 3, 4]\n",
      "Shuffled: [47839, 2446, 15669, 39639, 56293]\n",
      "--------- INDEX shuffled ---------\n",
      "\n",
      "Total number of samples: 60000 datapoints\n",
      "Number of train samples: 750 batches/ 48000 datapoints\n",
      "Number of val samples: 94 batches/ 6000 datapoints\n",
      "Number of test samples: 94 batches/ 6000 datapoints\n",
      "\n",
      "test lab1_lr01\n",
      "model total parameters: 50,890\n",
      "learning rate=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:12<00:00, 61.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], phase: train, samples: 48000, Loss: 0.0075, Top-1 Accuracy: 86.40%, Top-3 Accuracy: 96.18%\n",
      "Elapsed time: 12.16006588935852 seconds\n",
      "weight saved as: lab1_lr01.pt\n",
      "--------- INDEX checking ---------\n",
      "Original: [0, 1, 2, 3, 4]\n",
      "Shuffled: [56860, 3873, 2040, 45897, 42529]\n",
      "--------- INDEX shuffled ---------\n",
      "\n",
      "Total number of samples: 60000 datapoints\n",
      "Number of train samples: 750 batches/ 48000 datapoints\n",
      "Number of val samples: 94 batches/ 6000 datapoints\n",
      "Number of test samples: 94 batches/ 6000 datapoints\n",
      "\n",
      "test lab1_lr25\n",
      "model total parameters: 50,890\n",
      "learning rate=0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:11<00:00, 63.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], phase: train, samples: 48000, Loss: 0.0125, Top-1 Accuracy: 75.68%, Top-3 Accuracy: 92.19%\n",
      "Elapsed time: 11.829688310623169 seconds\n",
      "weight saved as: lab1_lr25.pt\n",
      "--------- INDEX checking ---------\n",
      "Original: [0, 1, 2, 3, 4]\n",
      "Shuffled: [57218, 37927, 15002, 33177, 48018]\n",
      "--------- INDEX shuffled ---------\n",
      "\n",
      "Total number of samples: 60000 datapoints\n",
      "Number of train samples: 12000 batches/ 48000 datapoints\n",
      "Number of val samples: 1500 batches/ 6000 datapoints\n",
      "Number of test samples: 1500 batches/ 6000 datapoints\n",
      "\n",
      "test lab2_BS0004\n",
      "model total parameters: 50,890\n",
      "learning rate=0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12000/12000 [00:32<00:00, 364.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], phase: train, samples: 48000, Loss: 0.1333, Top-1 Accuracy: 84.02%, Top-3 Accuracy: 96.02%\n",
      "Elapsed time: 32.894978761672974 seconds\n",
      "weight saved as: lab2_BS0004.pt\n",
      "--------- INDEX checking ---------\n",
      "Original: [0, 1, 2, 3, 4]\n",
      "Shuffled: [49992, 59258, 9498, 51698, 29638]\n",
      "--------- INDEX shuffled ---------\n",
      "\n",
      "Total number of samples: 60000 datapoints\n",
      "Number of train samples: 47 batches/ 48000 datapoints\n",
      "Number of val samples: 6 batches/ 6000 datapoints\n",
      "Number of test samples: 6 batches/ 6000 datapoints\n",
      "\n",
      "test lab2_BS1024\n",
      "model total parameters: 50,890\n",
      "learning rate=0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:09<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], phase: train, samples: 48000, Loss: 0.0009, Top-1 Accuracy: 73.88%, Top-3 Accuracy: 89.26%\n",
      "Elapsed time: 9.189153909683228 seconds\n",
      "weight saved as: lab2_BS1024.pt\n",
      "--------- INDEX checking ---------\n",
      "Original: [0, 1, 2, 3, 4]\n",
      "Shuffled: [49165, 23404, 25121, 40240, 33522]\n",
      "--------- INDEX shuffled ---------\n",
      "\n",
      "Total number of samples: 60000 datapoints\n",
      "Number of train samples: 750 batches/ 48000 datapoints\n",
      "Number of val samples: 94 batches/ 6000 datapoints\n",
      "Number of test samples: 94 batches/ 6000 datapoints\n",
      "\n",
      "test lab3_softmax\n",
      "model total parameters: 50,890\n",
      "learning rate=0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/750 [00:00<?, ?it/s]C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_30564\\1436985451.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.activation(x)\n",
      "100%|██████████| 750/750 [00:11<00:00, 62.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], phase: train, samples: 48000, Loss: 0.0197, Top-1 Accuracy: 56.15%, Top-3 Accuracy: 92.13%\n",
      "Elapsed time: 11.909446477890015 seconds\n",
      "weight saved as: lab3_softmax.pt\n",
      "--------- INDEX checking ---------\n",
      "Original: [0, 1, 2, 3, 4]\n",
      "Shuffled: [8602, 14133, 21355, 57023, 17637]\n",
      "--------- INDEX shuffled ---------\n",
      "\n",
      "Total number of samples: 60000 datapoints\n",
      "Number of train samples: 750 batches/ 48000 datapoints\n",
      "Number of val samples: 94 batches/ 6000 datapoints\n",
      "Number of test samples: 94 batches/ 6000 datapoints\n",
      "\n",
      "test lab3_sigmoid\n",
      "model total parameters: 50,890\n",
      "learning rate=0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:12<00:00, 62.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], phase: train, samples: 48000, Loss: 0.0064, Top-1 Accuracy: 88.60%, Top-3 Accuracy: 96.98%\n",
      "Elapsed time: 12.079464673995972 seconds\n",
      "weight saved as: lab3_sigmoid.pt\n",
      "--------- INDEX checking ---------\n",
      "Original: [0, 1, 2, 3, 4]\n",
      "Shuffled: [35917, 36082, 31816, 53949, 43035]\n",
      "--------- INDEX shuffled ---------\n",
      "\n",
      "Total number of samples: 60000 datapoints\n",
      "Number of train samples: 750 batches/ 48000 datapoints\n",
      "Number of val samples: 94 batches/ 6000 datapoints\n",
      "Number of test samples: 94 batches/ 6000 datapoints\n",
      "\n",
      "test lab4_SGD\n",
      "model total parameters: 50,890\n",
      "learning rate=0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:12<00:00, 62.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], phase: train, samples: 48000, Loss: 0.0179, Top-1 Accuracy: 73.47%, Top-3 Accuracy: 90.65%\n",
      "Elapsed time: 12.000512838363647 seconds\n",
      "weight saved as: lab4_SGD.pt\n",
      "--------- INDEX checking ---------\n",
      "Original: [0, 1, 2, 3, 4]\n",
      "Shuffled: [48685, 16157, 46888, 3389, 9268]\n",
      "--------- INDEX shuffled ---------\n",
      "\n",
      "Total number of samples: 60000 datapoints\n",
      "Number of train samples: 750 batches/ 48000 datapoints\n",
      "Number of val samples: 94 batches/ 6000 datapoints\n",
      "Number of test samples: 94 batches/ 6000 datapoints\n",
      "\n",
      "test lab5_cnn\n",
      "model total parameters: 220,234\n",
      "learning rate=0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:16<00:00, 46.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], phase: train, samples: 48000, Loss: 0.0025, Top-1 Accuracy: 95.01%, Top-3 Accuracy: 98.72%\n",
      "Elapsed time: 16.01808524131775 seconds\n",
      "weight saved as: lab5_cnn.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = 'D:\\\\Casper\\\\OTHER\\\\Data\\\\MNIST_data'\n",
    "\n",
    "model_list = [\n",
    "    # lambda: SimpleCNN(),\n",
    "\n",
    "    lambda: SimpleNN(\"ReLU\"),\n",
    "\n",
    "    lambda: SimpleNN(\"ReLU\"),\n",
    "    lambda: SimpleNN(\"ReLU\"),\n",
    "\n",
    "    lambda: SimpleNN(\"ReLU\"),\n",
    "    lambda: SimpleNN(\"ReLU\"),\n",
    "\n",
    "    lambda: SimpleNN(\"softmax\"),\n",
    "    lambda: SimpleNN(\"sigmoid\"),\n",
    "\n",
    "    lambda: SimpleNN(\"ReLU\"),\n",
    "    \n",
    "    lambda: SimpleCNN(),\n",
    "]\n",
    "\n",
    "model_name = [\n",
    "    \"base\",\n",
    "\n",
    "    \"lab1_lr01\",\n",
    "    \"lab1_lr25\",\n",
    "\n",
    "    \"lab2_BS0004\",\n",
    "    \"lab2_BS1024\",\n",
    "\n",
    "    \"lab3_softmax\",\n",
    "    \"lab3_sigmoid\",\n",
    "\n",
    "    \"lab4_SGD\",\n",
    "\n",
    "    \"lab5_cnn\"\n",
    "]\n",
    "loaders = [\n",
    "    lambda: load_original_data_pytorch(path, 0.8, 0.5, 64),\n",
    "\n",
    "    lambda: load_original_data_pytorch(path, 0.8, 0.5, 64),\n",
    "    lambda: load_original_data_pytorch(path, 0.8, 0.5, 64),\n",
    "\n",
    "    lambda: load_original_data_pytorch(path, 0.8, 0.5, 4),\n",
    "    lambda: load_original_data_pytorch(path, 0.8, 0.5, 1024),\n",
    "\n",
    "    lambda: load_original_data_pytorch(path, 0.8, 0.5, 64),\n",
    "    lambda: load_original_data_pytorch(path, 0.8, 0.5, 64),\n",
    "\n",
    "    lambda: load_original_data_pytorch(path, 0.8, 0.5, 64),\n",
    "\n",
    "    lambda: load_original_data_pytorch(path, 0.8, 0.5, 64),\n",
    "\n",
    "]\n",
    "lr = [\n",
    "    0.005,\n",
    "\n",
    "    0.001,\n",
    "    0.025,\n",
    "\n",
    "    0.005,\n",
    "    0.005,\n",
    "\n",
    "    0.005,\n",
    "    0.005,\n",
    "\n",
    "    0.005,\n",
    "\n",
    "    0.005,\n",
    "]\n",
    "optimizers = [\n",
    "    0,\n",
    "\n",
    "    0,\n",
    "    0,\n",
    "\n",
    "    0,\n",
    "    0,\n",
    "\n",
    "    0,\n",
    "    0,\n",
    "\n",
    "    1,\n",
    "\n",
    "    0,\n",
    "]\n",
    "for ii in range(len(model_name)):\n",
    "    train(model_list[ii], model_name[ii], loaders[ii], optimizers[ii], lr[ii], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
