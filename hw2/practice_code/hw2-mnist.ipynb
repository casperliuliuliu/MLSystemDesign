{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "from datetime import datetime\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import torchvision.models as models\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(dataset, train_ratio, val_ratio, batch_size):\n",
    "    train_dataset = dataset\n",
    "    val_dataset = dataset\n",
    "    test_dataset = dataset\n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(test_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    print(\"--------- INDEX checking ---------\")\n",
    "    print(f\"Original: {indices[:5]}\")\n",
    "    random.shuffle(indices)\n",
    "    print(f\"Shuffled: {indices[:5]}\")\n",
    "    print(\"--------- INDEX shuffled ---------\\n\")\n",
    "\n",
    "    split_train = int(np.floor(train_ratio * num_train))\n",
    "    split_val = split_train + int(np.floor(val_ratio * (num_train-split_train)))\n",
    "    train_idx, val_idx, test_idx = indices[0:split_train], indices[split_train:split_val], indices[split_val:]\n",
    "    merge_dataset = Subset(train_dataset, train_idx)\n",
    "\n",
    "    train_loader = DataLoader(merge_dataset, batch_size=batch_size)\n",
    "    val_loader = DataLoader(Subset(val_dataset, val_idx), batch_size=batch_size)\n",
    "    test_loader = DataLoader(Subset(test_dataset, test_idx), batch_size=batch_size)\n",
    "    \n",
    "    # check dataset\n",
    "    print(f\"Total number of samples: {num_train} datapoints\")\n",
    "    print(f\"Number of train samples: {len(train_loader)} batches/ {len(train_loader.dataset)} datapoints\")\n",
    "    print(f\"Number of val samples: {len(val_loader)} batches/ {len(val_loader.dataset)} datapoints\")\n",
    "    print(f\"Number of test samples: {len(test_loader)} batches/ {len(test_loader.dataset)} datapoints\")\n",
    "    print(f\"\")\n",
    "    \n",
    "    dataloaders = {\n",
    "        \"train\": train_loader,\n",
    "        \"val\": val_loader,\n",
    "        \"test\": test_loader,\n",
    "    }\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_original_data_pytorch(path, train_ratio, val_ratio, batchsize):\n",
    "    # Define a transform to normalize the data\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,), (0.5,))])\n",
    "    \n",
    "    # Download and load the training data\n",
    "    trainset = datasets.MNIST(path, download=True, train=True, transform=transform)\n",
    "    # train_loader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True)\n",
    "    dataloaders = get_dataloaders(trainset,train_ratio, val_ratio, batchsize)\n",
    "    # Download and load the test data\n",
    "    testset = datasets.MNIST(path, download=True, train=False, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batchsize, shuffle=True)\n",
    "\n",
    "    loaders = {\n",
    "    'train': dataloaders['train'],\n",
    "    'val': dataloaders['val'],\n",
    "    'test': test_loader,\n",
    "    }\n",
    "    return loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START LAB\n"
     ]
    }
   ],
   "source": [
    "def pprint(output = '\\n', show_time = False): # print and fprint at the same time\n",
    "    filename = \"hw2-1-MAR27.txt\"\n",
    "    print(output)\n",
    "    with open(filename, 'a') as f:\n",
    "        if show_time:\n",
    "            f.write(datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S] \"))\n",
    "\n",
    "        f.write(str(output))\n",
    "        f.write('\\n')\n",
    "pprint(\"START LAB\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_num = 0\n",
    "\n",
    "    for parameter in model.parameters():\n",
    "        if parameter.requires_grad:\n",
    "            total_num += parameter.numel() \n",
    "    return total_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, model_name, loaders, optim_op, lr, save_model=False):\n",
    "    model = model()\n",
    "    loaders = loaders()\n",
    "    pprint(f\"test {model_name}\", True)\n",
    "    model_parameters_amount = count_parameters(model)\n",
    "    pprint(f\"model total parameters: {model_parameters_amount:,}\")\n",
    "\n",
    "    model = model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if optim_op == 0:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optim_op == 1:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    pprint(f\"learning rate={lr}\")\n",
    "    iteration = 0\n",
    "    epochs = 10\n",
    "    start = time.time()\n",
    "    phases = ['train', 'val', 'test']\n",
    "    for epoch in range(epochs):\n",
    "        for phase in phases:\n",
    "            running_loss = 0.0\n",
    "            correct_predictions = 0\n",
    "            correct_top3_predictions = 0\n",
    "            total_samples = 0\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            for images, labels in tqdm(loaders[phase]): # Iterate over data.\n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    if phase == 'train': # backward + optimize only if in training phase\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Convert outputs to predicted class by selecting the class with the highest score\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                # Accumulate the number of correct predictions\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                \n",
    "                _, top3_preds = outputs.topk(3, 1, True, True)\n",
    "                correct_top3_predictions += sum([labels[i] in top3_preds[i] for i in range(labels.size(0))])\n",
    "\n",
    "                total_samples += labels.size(0)\n",
    "                iteration += 1\n",
    "                # if iteration % 20 == 0:\n",
    "                #     print(iteration)\n",
    "            avg_loss = running_loss / total_samples\n",
    "            top1_accuracy = correct_predictions / total_samples * 100\n",
    "            top3_accuracy = correct_top3_predictions / total_samples * 100\n",
    "            pprint(f\"Epoch [{epoch+1}/{epochs}], phase: {phase}, samples: {total_samples}, Loss: {avg_loss:.4f}, Top-1 Accuracy: {top1_accuracy:.2f}%, Top-3 Accuracy: {top3_accuracy:.2f}%\")\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    pprint(f\"Elapsed time: {duration} seconds\")\n",
    "    if save_model:\n",
    "        model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "        model_scripted.save(f'{model_name}.pt') # Save\n",
    "        pprint(f\"weight saved as: {model_name}.pt\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, act_layer):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "        if act_layer == \"softmax\":\n",
    "            self.activation = F.softmax\n",
    "        elif act_layer == \"sigmoid\":\n",
    "            self.activation = F.sigmoid\n",
    "        elif act_layer == \"ReLU\":\n",
    "            self.activation = F.relu\n",
    "        elif act_layer == \"leakyReLU\":\n",
    "            self.activation = F.leaky_relu\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the input tensor\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 64)  # The image size is reduced to 7x7 after pooling layers\n",
    "        self.fc2 = nn.Linear(64, 10)  # 10 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Convolution -> ReLU -> Pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Convolution -> ReLU -> Pooling\n",
    "        x = torch.flatten(x, 1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))  # Dense layer -> ReLU\n",
    "        x = self.fc2(x)  # Output layer\n",
    "        return F.log_softmax(x, dim=1)  # Log Softmax activation for the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- INDEX checking ---------\n",
      "Original: [0, 1, 2, 3, 4]\n",
      "Shuffled: [33681, 21979, 54846, 20344, 3999]\n",
      "--------- INDEX shuffled ---------\n",
      "\n",
      "Total number of samples: 60000 datapoints\n",
      "Number of train samples: 750 batches/ 48000 datapoints\n",
      "Number of val samples: 94 batches/ 6000 datapoints\n",
      "Number of test samples: 94 batches/ 6000 datapoints\n",
      "\n",
      "test base\n",
      "model total parameters: 50,890\n",
      "learning rate=0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/750 [00:00<00:09, 81.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:11<00:00, 64.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], phase: train, samples: 48000, Loss: 0.0066, Top-1 Accuracy: 87.01%, Top-3 Accuracy: 96.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:01<00:00, 80.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], phase: val, samples: 6000, Loss: 0.0044, Top-1 Accuracy: 91.75%, Top-3 Accuracy: 98.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 68.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], phase: test, samples: 10000, Loss: 0.0046, Top-1 Accuracy: 90.87%, Top-3 Accuracy: 98.39%\n",
      "Elapsed time: 15.076114177703857 seconds\n",
      "--------- INDEX checking ---------\n",
      "Original: [0, 1, 2, 3, 4]\n",
      "Shuffled: [2342, 22855, 29889, 2548, 1295]\n",
      "--------- INDEX shuffled ---------\n",
      "\n",
      "Total number of samples: 60000 datapoints\n",
      "Number of train samples: 750 batches/ 48000 datapoints\n",
      "Number of val samples: 94 batches/ 6000 datapoints\n",
      "Number of test samples: 94 batches/ 6000 datapoints\n",
      "\n",
      "test lab1_lr01\n",
      "model total parameters: 50,890\n",
      "learning rate=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:11<00:00, 62.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], phase: train, samples: 48000, Loss: 0.0073, Top-1 Accuracy: 86.69%, Top-3 Accuracy: 96.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:01<00:00, 74.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], phase: val, samples: 6000, Loss: 0.0051, Top-1 Accuracy: 90.63%, Top-3 Accuracy: 98.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 24/157 [00:00<00:01, 72.12it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 88\u001b[0m\n\u001b[0;32m     71\u001b[0m optimizers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     73\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     86\u001b[0m ]\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(model_name)):\n\u001b[1;32m---> 88\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mii\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m[\u001b[49m\u001b[43mii\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mii\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mii\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mii\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 47\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, model_name, loaders, optim_op, lr, save_model)\u001b[0m\n\u001b[0;32m     44\u001b[0m correct_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     46\u001b[0m _, top3_preds \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mtopk(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 47\u001b[0m correct_top3_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([labels[i] \u001b[38;5;129;01min\u001b[39;00m top3_preds[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))])\n\u001b[0;32m     49\u001b[0m total_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     50\u001b[0m iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[17], line 47\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     44\u001b[0m correct_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     46\u001b[0m _, top3_preds \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mtopk(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 47\u001b[0m correct_top3_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtop3_preds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))])\n\u001b[0;32m     49\u001b[0m total_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     50\u001b[0m iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\_tensor.py:1059\u001b[0m, in \u001b[0;36mTensor.__contains__\u001b[1;34m(self, element)\u001b[0m\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__contains__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, element)\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m   1056\u001b[0m     element, (torch\u001b[38;5;241m.\u001b[39mTensor, Number, torch\u001b[38;5;241m.\u001b[39mSymInt, torch\u001b[38;5;241m.\u001b[39mSymFloat, torch\u001b[38;5;241m.\u001b[39mSymBool)\n\u001b[0;32m   1057\u001b[0m ):\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;66;03m# type hint doesn't understand the __contains__ result array\u001b[39;00m\n\u001b[1;32m-> 1059\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor.__contains__ only supports Tensor or scalar, but you passed in a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(element)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1063\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = 'D:\\\\Casper\\\\OTHER\\\\Data\\\\MNIST_data'\n",
    "\n",
    "model_list = [\n",
    "    # lambda: SimpleCNN(),\n",
    "\n",
    "    lambda: SimpleNN(\"ReLU\"),\n",
    "\n",
    "    lambda: SimpleNN(\"ReLU\"),\n",
    "    lambda: SimpleNN(\"ReLU\"),\n",
    "\n",
    "    lambda: SimpleNN(\"ReLU\"),\n",
    "    lambda: SimpleNN(\"ReLU\"),\n",
    "\n",
    "    lambda: SimpleNN(\"softmax\"),\n",
    "    lambda: SimpleNN(\"sigmoid\"),\n",
    "\n",
    "    lambda: SimpleNN(\"ReLU\"),\n",
    "    \n",
    "    lambda: SimpleCNN(),\n",
    "]\n",
    "\n",
    "model_name = [\n",
    "    \"base\",\n",
    "\n",
    "    \"lab1_lr01\",\n",
    "    \"lab1_lr25\",\n",
    "\n",
    "    \"lab2_BS0004\",\n",
    "    \"lab2_BS1024\",\n",
    "\n",
    "    \"lab3_softmax\",\n",
    "    \"lab3_sigmoid\",\n",
    "\n",
    "    \"lab4_SGD\",\n",
    "\n",
    "    \"lab5_cnn\"\n",
    "]\n",
    "loaders = [\n",
    "    lambda: load_original_data_pytorch(path, 0.8, 0.5, 64),\n",
    "\n",
    "    lambda: load_original_data_pytorch(path, 0.8, 0.5, 64),\n",
    "    lambda: load_original_data_pytorch(path, 0.8, 0.5, 64),\n",
    "\n",
    "    lambda: load_original_data_pytorch(path, 0.8, 0.5, 4),\n",
    "    lambda: load_original_data_pytorch(path, 0.8, 0.5, 1024),\n",
    "\n",
    "    lambda: load_original_data_pytorch(path, 0.8, 0.5, 64),\n",
    "    lambda: load_original_data_pytorch(path, 0.8, 0.5, 64),\n",
    "\n",
    "    lambda: load_original_data_pytorch(path, 0.8, 0.5, 64),\n",
    "\n",
    "    lambda: load_original_data_pytorch(path, 0.8, 0.5, 64),\n",
    "\n",
    "]\n",
    "lr = [\n",
    "    0.005,\n",
    "\n",
    "    0.001,\n",
    "    0.025,\n",
    "\n",
    "    0.005,\n",
    "    0.005,\n",
    "\n",
    "    0.005,\n",
    "    0.005,\n",
    "\n",
    "    0.005,\n",
    "\n",
    "    0.005,\n",
    "]\n",
    "optimizers = [\n",
    "    0,\n",
    "\n",
    "    0,\n",
    "    0,\n",
    "\n",
    "    0,\n",
    "    0,\n",
    "\n",
    "    0,\n",
    "    0,\n",
    "\n",
    "    1,\n",
    "\n",
    "    0,\n",
    "]\n",
    "for ii in range(len(model_name)):\n",
    "    train(model_list[ii], model_name[ii], loaders[ii], optimizers[ii], lr[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
