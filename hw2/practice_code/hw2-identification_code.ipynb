{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import scipy.io\n",
    "from PIL import Image \n",
    "import os\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "from torch import optim, nn\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(path_X, path_Y, num_data, data_mode=\"train\"):\n",
    "    data = scipy.io.loadmat(path_X) \n",
    "    print(data.keys())\n",
    "    if data_mode == \"train\" or data_mode == \"test\":\n",
    "        origin_X = np.array(data['x'].flat) # train\n",
    "    elif data_mode == \"dn\":\n",
    "        origin_X = np.array(data['denoise_x']) # denoise train\n",
    "    elif data_mode == \"dn2\":\n",
    "        origin_X = np.array(data['denoise2_x']) # denoise train\n",
    "\n",
    "    data = scipy.io.loadmat(path_Y) \n",
    "    origin_Y = data['y'][0].reshape(num_data,-1)\n",
    "    origin_Y_onehot= data['y_onehot'].reshape(num_data,4,19)\n",
    "    \n",
    "    print (\"origin_X shape: \"+str(origin_X.shape))\n",
    "    print (\"origin_Y shape: \"+str(origin_Y.shape))\n",
    "    print (\"origin_Y_onehot shape: \"+str(origin_Y_onehot.shape))\n",
    " \n",
    "    return origin_X,origin_Y,origin_Y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img (o_data,write,save):  \n",
    "    index=0\n",
    "    p_data=[]\n",
    "    for i in o_data:\n",
    "        name='resize_data_image/resize_x_'+str(index)+'.jpg'\n",
    "        img = Image.fromarray(i, 'RGB')\n",
    "        img=img.resize((130,50))\n",
    "        if os.path.isfile(name) and save:      \n",
    "            print (name+\" is existed\")    \n",
    "        elif save:\n",
    "            img.save(name)\n",
    "        if write:\n",
    "            p_data.append(np.array(img))       \n",
    "        index+=1\n",
    "        \n",
    "    p_data=np.array(p_data)   \n",
    "    print (p_data.shape)\n",
    "    return p_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_data(mode=\"train\"):\n",
    "    num_data = 5000\n",
    "    data_mode = mode\n",
    "    path = \"D:\\\\Casper\\\\OTHER\\\\Data\\\\identification code_database\\\\train.mat\"\n",
    "    if data_mode == \"train\":\n",
    "        path2 = \"D:\\\\Casper\\\\OTHER\\\\Data\\\\identification code_database\\\\train.mat\"\n",
    "    elif data_mode == \"dn\":\n",
    "        path2 = \"D:\\\\Casper\\\\OTHER\\\\Data\\\\identification code_database\\\\denoise_train.mat\"\n",
    "    elif data_mode == \"dn2\":\n",
    "        path2 = \"D:\\\\Casper\\\\OTHER\\\\Data\\\\identification code_database\\\\denoise_train2.mat\"\n",
    "    elif data_mode == \"test\":\n",
    "        num_data = 3000\n",
    "        path = \"D:\\\\Casper\\\\OTHER\\\\Data\\\\identification code_database\\\\test.mat\"\n",
    "        path2 = \"D:\\\\Casper\\\\OTHER\\\\Data\\\\identification code_database\\\\test.mat\"\n",
    "\n",
    "    train_rate=1 #change to 0.9\n",
    "    origin_X,origin_Y,origin_Y_onehot=load_data(path2, path, num_data, data_mode)\n",
    "    num_train_data=int(num_data*train_rate)\n",
    "    print(origin_X.shape)\n",
    "\n",
    "    if data_mode == \"train\" or data_mode == \"test\":\n",
    "        resize_x = resize_img(origin_X,True,False) # train\n",
    "    elif data_mode == \"dn\":\n",
    "        resize_x = origin_X # denoise train\n",
    "    elif data_mode == \"dn2\":\n",
    "        resize_x = origin_X # denoise train\n",
    "    print(num_data)\n",
    "    train_x_orig=resize_x.reshape(num_data,50,130,-1)[0:num_train_data]\n",
    "    # test_x_orig=resize_x.reshape(num_data,50,130,-1)[num_train_data:]\n",
    "\n",
    "    x_train=train_x_orig.astype('float32')/255\n",
    "    # x_test=test_x_orig.astype('float32')/255\n",
    "\n",
    "    y_train_onehot=origin_Y_onehot[0:num_train_data]\n",
    "    # y_test_onehot=origin_Y_onehot[num_train_data:]\n",
    "    origin_X_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "    origin_Y_tensor = torch.tensor(y_train_onehot, dtype=torch.float32)\n",
    "\n",
    "    origin_X_tensor_permuted = origin_X_tensor.permute(0, 3, 1, 2)\n",
    "    train_X = origin_X_tensor_permuted\n",
    "    train_Y = torch.argmax(origin_Y_tensor, dim=-1)\n",
    "    dataset = TensorDataset(train_X, train_Y)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_channel):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channel, 32, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
    "        self.fc1 = nn.Linear(24576, 512)\n",
    "        self.fc2 = nn.Linear(512, 76)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BASELINE(nn.Module):\n",
    "    def __init__(self, input_channel=1):\n",
    "        super(BASELINE, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv11_W1 = nn.Conv2d(in_channels=input_channel, out_channels=64, kernel_size=5, stride=1, padding='same')\n",
    "        self.conv12_W1 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.max_pool1_W1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv23_W1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding='same')\n",
    "        self.conv24_W1 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1)\n",
    "        self.conv25_W1 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1)\n",
    "        self.bn1_W1 = nn.BatchNorm2d(128)\n",
    "        self.max_pool2_W1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv36_W1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=1, stride=1, padding='same')\n",
    "        self.conv37_W1 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1)\n",
    "        self.conv38_W1 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1)\n",
    "        self.max_pool3_W1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv49_W1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding='same')\n",
    "        self.conv410_W1 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=1, stride=1)\n",
    "        self.bn2_W1 = nn.BatchNorm2d(512)\n",
    "        self.max_pool4_W1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # Dense (Fully Connected) Layers for each output branch\n",
    "        self.fc_branches = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_features=3072, out_features=128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.25),\n",
    "                nn.Linear(in_features=128, out_features=128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=128, out_features=19),\n",
    "                nn.Softmax(dim=1)\n",
    "            ) for _ in range(4)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv11_W1(x))\n",
    "        x = F.relu(self.conv12_W1(x))\n",
    "        x = self.max_pool1_W1(x)\n",
    "        \n",
    "        x = F.relu(self.conv23_W1(x))\n",
    "        x = F.relu(self.conv24_W1(x))\n",
    "        x = F.relu(self.conv25_W1(x))\n",
    "        x = self.bn1_W1(x)\n",
    "        x = self.max_pool2_W1(x)\n",
    "        \n",
    "        x = F.relu(self.conv36_W1(x))\n",
    "        x = F.relu(self.conv37_W1(x))\n",
    "        x = F.relu(self.conv38_W1(x))\n",
    "        x = self.max_pool3_W1(x)\n",
    "        \n",
    "        x = F.relu(self.conv49_W1(x))\n",
    "        x = F.relu(self.conv410_W1(x))\n",
    "        x = self.bn2_W1(x)\n",
    "        x = self.max_pool4_W1(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Branch out to the four different dense layers\n",
    "        outputs = torch.stack([branch(x) for branch in self.fc_branches], dim = 1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dataloaders(dataset, train_ratio, val_ratio, batch_size):\n",
    "    train_dataset = dataset\n",
    "    val_dataset = dataset\n",
    "    test_dataset = dataset\n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(test_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    print(\"--------- INDEX checking ---------\")\n",
    "    print(f\"Original: {indices[:5]}\")\n",
    "    random.shuffle(indices)\n",
    "    print(f\"Shuffled: {indices[:5]}\")\n",
    "    print(\"--------- INDEX shuffled ---------\\n\")\n",
    "\n",
    "    split_train = int(np.floor(train_ratio * num_train))\n",
    "    split_val = split_train + int(np.floor(val_ratio * (num_train-split_train)))\n",
    "    train_idx, val_idx, test_idx = indices[0:split_train], indices[split_train:split_val], indices[split_val:]\n",
    "    merge_dataset = Subset(train_dataset, train_idx)\n",
    "\n",
    "    train_loader = DataLoader(merge_dataset, batch_size=batch_size)\n",
    "    val_loader = DataLoader(Subset(val_dataset, val_idx), batch_size=batch_size)\n",
    "    test_loader = DataLoader(Subset(test_dataset, test_idx), batch_size=batch_size)\n",
    "    \n",
    "    # check dataset\n",
    "    print(f\"Total number of samples: {num_train} datapoints\")\n",
    "    print(f\"Number of train samples: {len(train_loader)} batches/ {len(train_loader.dataset)} datapoints\")\n",
    "    print(f\"Number of val samples: {len(val_loader)} batches/ {len(val_loader.dataset)} datapoints\")\n",
    "    print(f\"Number of test samples: {len(test_loader)} batches/ {len(test_loader.dataset)} datapoints\")\n",
    "    print(f\"\")\n",
    "    \n",
    "    dataloaders = {\n",
    "        \"train\": train_loader,\n",
    "        \"val\": val_loader,\n",
    "        \"test\": test_loader,\n",
    "    }\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START function\n"
     ]
    }
   ],
   "source": [
    "def pprint(output = '\\n', show_time = False): # print and fprint at the same time\n",
    "    filename = \"hw2-2-MAR27.txt\"\n",
    "    print(output)\n",
    "    with open(filename, 'a') as f:\n",
    "        if show_time:\n",
    "            f.write(datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S] \"))\n",
    "\n",
    "        f.write(str(output))\n",
    "        f.write('\\n')\n",
    "pprint(\"START function\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_num = 0\n",
    "    for parameter in model.parameters():\n",
    "        if parameter.requires_grad:\n",
    "            total_num += parameter.numel() \n",
    "    return total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_lists, model_name, loaders, phases=['train'], reshape=True, save_weight=False):\n",
    "    model = model_lists[model_name]()\n",
    "    if \"res\" in model_name:\n",
    "        # model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) # denoise train\n",
    "        num_features = model.fc.in_features\n",
    "        model.fc = torch.nn.Linear(num_features, 76)\n",
    "\n",
    "    pprint(f\"Training model: {model_name}\")\n",
    "    model_parameters_amount = count_parameters(model)  # Assume this function is defined elsewhere\n",
    "    pprint(f\"Total parameters: {model_parameters_amount:,}\")\n",
    "\n",
    "    model = model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lr = 0.001\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    pprint(f\"Learning rate={lr}\")\n",
    "    epochs = 25\n",
    "\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        for phase in phases:\n",
    "            running_loss = 0.0\n",
    "            correct_predictions = [0, 0, 0, 0]  # Track correct predictions for each of the 4 targets\n",
    "            total_samples = 0\n",
    "            model.train() if phase == 'train' else model.eval()  # Simplified model mode setting\n",
    "\n",
    "            for inputs, labels in tqdm(loaders[phase]):  # Iterate over data.\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)  # [batch_size, 4, 19]\n",
    "                    if reshape:\n",
    "                        outputs = outputs.reshape(labels.shape[0], 4, -1)\n",
    "                    loss = sum([criterion(outputs[:, i, :], labels[:, i]) for i in range(4)])  # Sum loss across all targets\n",
    "\n",
    "                    if phase == 'train':  # backward + optimize only if in training phase\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                for i in range(4):\n",
    "                    _, predicted = torch.max(outputs[:, i, :], 1)\n",
    "                    correct_predictions[i] += (predicted == labels[:, i]).sum().item()\n",
    "\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "            avg_loss = running_loss / total_samples\n",
    "            top1_accuracy = [cp / total_samples * 100 for cp in correct_predictions]  # Accuracy per target\n",
    "            pprint(f\"Epoch [{epoch+1}/{epochs}], phase: {phase}, samples: {total_samples}, Loss: {avg_loss:.4f}, \"\n",
    "                  f\"Top-1 Accuracies: {[f'{acc:.2f}%' for acc in top1_accuracy]}\")\n",
    "\n",
    "    end = time.time()\n",
    "    pprint(f\"Elapsed time: {end - start} seconds\")\n",
    "\n",
    "    if save_weight:\n",
    "        model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "        model_scripted.save(f'{model_name}.pt') # Save\n",
    "        pprint(f\"weight saved as: {model_name}.pt\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'y_onehot', 'x', 'y'])\n",
      "origin_X shape: (3000,)\n",
      "origin_Y shape: (3000, 4)\n",
      "origin_Y_onehot shape: (3000, 4, 19)\n",
      "(3000,)\n",
      "(3000, 50, 130, 3)\n",
      "3000\n",
      "--------- INDEX checking ---------\n",
      "Original: [0, 1, 2, 3, 4]\n",
      "Shuffled: [908, 110, 2493, 866, 1720]\n",
      "--------- INDEX shuffled ---------\n",
      "\n",
      "Total number of samples: 3000 datapoints\n",
      "Number of train samples: 94 batches/ 3000 datapoints\n",
      "Number of val samples: 0 batches/ 0 datapoints\n",
      "Number of test samples: 0 batches/ 0 datapoints\n",
      "\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'y_onehot', 'x', 'y'])\n",
      "origin_X shape: (5000,)\n",
      "origin_Y shape: (5000, 4)\n",
      "origin_Y_onehot shape: (5000, 4, 19)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "data_lists = [\n",
    "    'train',\n",
    "    'dn',\n",
    "    'dn2',\n",
    "]\n",
    "test_dataset = get_data('test')\n",
    "test_loaders = get_dataloaders(test_dataset, 1, 0.5, 32)\n",
    "test_loader = test_loaders['train']\n",
    "\n",
    "input_channel_op = 1\n",
    "\n",
    "model_list ={\n",
    "    \"BASELINE\": lambda: BASELINE(input_channel=input_channel_op),\n",
    "    \"SimpleCNN\": lambda: SimpleCNN(input_channel=input_channel_op),\n",
    "    \"resnet18_mod\": lambda: mod_resnet(BasicBlock, [2, 2, 2, 2], channel_num_list=[16, 16, 16, 32, 32], num_classes=76, input_channel=input_channel_op),\n",
    "    \"resnet18\": lambda: mod_resnet(BasicBlock, [2, 2, 2, 2], channel_num_list=[64, 64, 128, 256, 512], num_classes=76, input_channel=input_channel_op),\n",
    "}\n",
    "model_names = [\n",
    "    \"BASELINE\",\n",
    "    \"SimpleCNN\",\n",
    "    \"resnet18\",\n",
    "    \"resnet18\",\n",
    "]\n",
    "reshape_ops = [\n",
    "    False,\n",
    "    True,\n",
    "    True,\n",
    "    True,\n",
    "]\n",
    "\n",
    "phases = ['train', 'val', 'test']\n",
    "\n",
    "\n",
    "for ii in range(9, 12):\n",
    "    data_mode = data_lists[ii%3]\n",
    "    model_name = model_names[ii//3]\n",
    "    reshape_op = reshape_ops[ii//3]\n",
    "\n",
    "    train_dataset = get_data(data_mode)\n",
    "    loaders = get_dataloaders(train_dataset, 0.8, 1, 32)\n",
    "    \n",
    "    if data_mode == 'train':\n",
    "        input_channel_op = 3\n",
    "        loaders['test'] = test_loader\n",
    "        phases = ['train', 'val', 'test']\n",
    "        saving_weight = True\n",
    "    else:\n",
    "        input_channel_op = 1\n",
    "        phases = ['train', 'val']\n",
    "        saving_weight = False\n",
    "\n",
    "    train(model_list, model_name, loaders, phases, reshape_op, saving_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_list ={\n",
    "#     \"SimpleCharCNN\": lambda: SimpleCharCNN(),\n",
    "#     \"resnet18\": lambda: models.resnet18(weights = models.ResNet18_Weights.DEFAULT),\n",
    "#     \"resnet152\": lambda: models.resnet152(weights = models.ResNet152_Weights.DEFAULT),\n",
    "#     # \"r6_btnk\": lambda: mod_resnet(Bottleneck, [2, 2, 0, 0], channel_num_list=[16, 16, 16], num_classes=76)\n",
    "#     \"r6_btnk\": lambda: mod_resnet(Bottleneck, [2, 2, 0, 0], channel_num_list=[8, 4, 8], num_classes=76)\n",
    "# }\n",
    "# model_name = \"r6_btnk\"\n",
    "# phases = ['train', 'val']\n",
    "# loaders = get_dataloaders(train_dataset, 0.8, 0.5, 32)\n",
    "# train(model_list, model_name, loaders, phases, reshape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "class mod_resnet(nn.Module):\n",
    "    def __init__(self, block, layers, channel_num_list, num_classes=1000, input_channel=3):\n",
    "        super(mod_resnet, self).__init__()\n",
    "        self.in_channels = channel_num_list[0]\n",
    "        self.conv1 = nn.Conv2d(input_channel, channel_num_list[0], kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channel_num_list[0])\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, channel_num_list[1], layers[0])\n",
    "        self.layer2 = self._make_layer(block, channel_num_list[2], layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, channel_num_list[3], layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, channel_num_list[4], layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(channel_num_list[-1] * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
