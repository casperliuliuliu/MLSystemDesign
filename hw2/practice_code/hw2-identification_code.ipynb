{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "from datetime import datetime\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import scipy.io\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image \n",
    "import os\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(path_X, path_Y, num_data, data_mode=\"train\"):\n",
    "    data = scipy.io.loadmat(path_X) \n",
    "    print(data.keys())\n",
    "    if data_mode == \"train\" or data_mode == \"test\":\n",
    "        origin_X = np.array(data['x'].flat) # train\n",
    "    elif data_mode == \"dn\":\n",
    "        origin_X = np.array(data['denoise_x']) # denoise train\n",
    "    elif data_mode == \"dn2\":\n",
    "        origin_X = np.array(data['denoise2_x']) # denoise train\n",
    "\n",
    "    data = scipy.io.loadmat(path_Y) \n",
    "    origin_Y = data['y'][0].reshape(num_data,-1)\n",
    "    origin_Y_onehot= data['y_onehot'].reshape(num_data,4,19)\n",
    "    \n",
    "    print (\"origin_X shape: \"+str(origin_X.shape))\n",
    "    print (\"origin_Y shape: \"+str(origin_Y.shape))\n",
    "    print (\"origin_Y_onehot shape: \"+str(origin_Y_onehot.shape))\n",
    " \n",
    "    return origin_X,origin_Y,origin_Y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img (o_data,write,save):  \n",
    "    index=0\n",
    "    p_data=[]\n",
    "    for i in o_data:\n",
    "        name='resize_data_image/resize_x_'+str(index)+'.jpg'\n",
    "        img = Image.fromarray(i, 'RGB')\n",
    "        img=img.resize((130,50))\n",
    "        if os.path.isfile(name) and save:      \n",
    "            print (name+\" is existed\")    \n",
    "        elif save:\n",
    "            img.save(name)\n",
    "        if write:\n",
    "            p_data.append(np.array(img))       \n",
    "        index+=1\n",
    "        \n",
    "    p_data=np.array(p_data)   \n",
    "    print (p_data.shape)\n",
    "    return p_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_data(mode=\"train\"):\n",
    "    num_data = 5000\n",
    "    data_mode = mode\n",
    "    path = \"D:\\\\Casper\\\\OTHER\\\\Data\\\\identification code_database\\\\train.mat\"\n",
    "    if data_mode == \"train\":\n",
    "        path2 = \"D:\\\\Casper\\\\OTHER\\\\Data\\\\identification code_database\\\\train.mat\"\n",
    "    elif data_mode == \"dn\":\n",
    "        path2 = \"D:\\\\Casper\\\\OTHER\\\\Data\\\\identification code_database\\\\denoise_train.mat\"\n",
    "    elif data_mode == \"dn2\":\n",
    "        path2 = \"D:\\\\Casper\\\\OTHER\\\\Data\\\\identification code_database\\\\denoise_train2.mat\"\n",
    "    elif data_mode == \"test\":\n",
    "        num_data = 3000\n",
    "        path = \"D:\\\\Casper\\\\OTHER\\\\Data\\\\identification code_database\\\\test.mat\"\n",
    "        path2 = \"D:\\\\Casper\\\\OTHER\\\\Data\\\\identification code_database\\\\test.mat\"\n",
    "\n",
    "    train_rate=1 #change to 0.9\n",
    "    origin_X,origin_Y,origin_Y_onehot=load_data(path2, path, num_data, data_mode)\n",
    "    num_train_data=int(num_data*train_rate)\n",
    "    print(origin_X.shape)\n",
    "\n",
    "    if data_mode == \"train\" or data_mode == \"test\":\n",
    "        resize_x = resize_img(origin_X,True,False) # train\n",
    "    elif data_mode == \"dn\":\n",
    "        resize_x = origin_X # denoise train\n",
    "    elif data_mode == \"dn2\":\n",
    "        resize_x = origin_X # denoise train\n",
    "    print(num_data)\n",
    "    train_x_orig=resize_x.reshape(num_data,50,130,-1)[0:num_train_data]\n",
    "    # test_x_orig=resize_x.reshape(num_data,50,130,-1)[num_train_data:]\n",
    "\n",
    "    x_train=train_x_orig.astype('float32')/255\n",
    "    # x_test=test_x_orig.astype('float32')/255\n",
    "\n",
    "    y_train_onehot=origin_Y_onehot[0:num_train_data]\n",
    "    # y_test_onehot=origin_Y_onehot[num_train_data:]\n",
    "    origin_X_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "    origin_Y_tensor = torch.tensor(y_train_onehot, dtype=torch.float32)\n",
    "\n",
    "    origin_X_tensor_permuted = origin_X_tensor.permute(0, 3, 1, 2)\n",
    "    train_X = origin_X_tensor_permuted\n",
    "    train_Y = torch.argmax(origin_Y_tensor, dim=-1)\n",
    "    dataset = TensorDataset(train_X, train_Y)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleCharCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCharCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, padding=2)  # Assuming grayscale images\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
    "        self.fc1 = nn.Linear(24576, 512)  # Adjust the size according to your image size\n",
    "        self.fc2 = nn.Linear(512, 76)  # 4 characters, each 26 possible letters\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCharCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCharCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv11_W1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=5, stride=1, padding='same')\n",
    "        self.conv12_W1 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.max_pool1_W1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv23_W1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding='same')\n",
    "        self.conv24_W1 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1)\n",
    "        self.conv25_W1 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1)\n",
    "        self.bn1_W1 = nn.BatchNorm2d(128)\n",
    "        self.max_pool2_W1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv36_W1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=1, stride=1, padding='same')\n",
    "        self.conv37_W1 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1)\n",
    "        self.conv38_W1 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1)\n",
    "        self.max_pool3_W1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv49_W1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding='same')\n",
    "        self.conv410_W1 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=1, stride=1)\n",
    "        self.bn2_W1 = nn.BatchNorm2d(512)\n",
    "        self.max_pool4_W1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # Dense (Fully Connected) Layers for each output branch\n",
    "        self.fc_branches = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_features=3072, out_features=128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.25),\n",
    "                nn.Linear(in_features=128, out_features=128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=128, out_features=19),\n",
    "                nn.Softmax(dim=1)\n",
    "            ) for _ in range(4)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv11_W1(x))\n",
    "        x = F.relu(self.conv12_W1(x))\n",
    "        x = self.max_pool1_W1(x)\n",
    "        \n",
    "        x = F.relu(self.conv23_W1(x))\n",
    "        x = F.relu(self.conv24_W1(x))\n",
    "        x = F.relu(self.conv25_W1(x))\n",
    "        x = self.bn1_W1(x)\n",
    "        x = self.max_pool2_W1(x)\n",
    "        \n",
    "        x = F.relu(self.conv36_W1(x))\n",
    "        x = F.relu(self.conv37_W1(x))\n",
    "        x = F.relu(self.conv38_W1(x))\n",
    "        x = self.max_pool3_W1(x)\n",
    "        \n",
    "        x = F.relu(self.conv49_W1(x))\n",
    "        x = F.relu(self.conv410_W1(x))\n",
    "        x = self.bn2_W1(x)\n",
    "        x = self.max_pool4_W1(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Branch out to the four different dense layers\n",
    "        outputs = torch.stack([branch(x) for branch in self.fc_branches], dim = 1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import time\n",
    "def get_dataloaders(dataset, train_ratio, val_ratio, batch_size):\n",
    "    train_dataset = dataset\n",
    "    val_dataset = dataset\n",
    "    test_dataset = dataset\n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(test_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    print(\"--------- INDEX checking ---------\")\n",
    "    print(f\"Original: {indices[:5]}\")\n",
    "    random.shuffle(indices)\n",
    "    print(f\"Shuffled: {indices[:5]}\")\n",
    "    print(\"--------- INDEX shuffled ---------\\n\")\n",
    "\n",
    "    split_train = int(np.floor(train_ratio * num_train))\n",
    "    split_val = split_train + int(np.floor(val_ratio * (num_train-split_train)))\n",
    "    train_idx, val_idx, test_idx = indices[0:split_train], indices[split_train:split_val], indices[split_val:]\n",
    "    merge_dataset = Subset(train_dataset, train_idx)\n",
    "\n",
    "    train_loader = DataLoader(merge_dataset, batch_size=batch_size)\n",
    "    val_loader = DataLoader(Subset(val_dataset, val_idx), batch_size=batch_size)\n",
    "    test_loader = DataLoader(Subset(test_dataset, test_idx), batch_size=batch_size)\n",
    "    \n",
    "    # check dataset\n",
    "    print(f\"Total number of samples: {num_train} datapoints\")\n",
    "    print(f\"Number of train samples: {len(train_loader)} batches/ {len(train_loader.dataset)} datapoints\")\n",
    "    print(f\"Number of val samples: {len(val_loader)} batches/ {len(val_loader.dataset)} datapoints\")\n",
    "    print(f\"Number of test samples: {len(test_loader)} batches/ {len(test_loader.dataset)} datapoints\")\n",
    "    print(f\"\")\n",
    "    \n",
    "    dataloaders = {\n",
    "        \"train\": train_loader,\n",
    "        \"val\": val_loader,\n",
    "        \"test\": test_loader,\n",
    "    }\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build function\n"
     ]
    }
   ],
   "source": [
    "def pprint(output = '\\n', show_time = False): # print and fprint at the same time\n",
    "    filename = \"hw2-2-MAR27.txt\"\n",
    "    print(output)\n",
    "    with open(filename, 'a') as f:\n",
    "        if show_time:\n",
    "            f.write(datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S] \"))\n",
    "\n",
    "        f.write(str(output))\n",
    "        f.write('\\n')\n",
    "pprint(\"build function\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_num = 0\n",
    "    for parameter in model.parameters():\n",
    "        if parameter.requires_grad:\n",
    "            total_num += parameter.numel() \n",
    "    return total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_lists, model_name, loaders, phases=['train'], reshape=False, save_weight=False):\n",
    "    model = model_lists[model_name]()\n",
    "    if \"res\" in model_name:\n",
    "        # model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) # denoise train\n",
    "        num_features = model.fc.in_features\n",
    "        model.fc = torch.nn.Linear(num_features, 76)\n",
    "\n",
    "    pprint(f\"Training model: {model_name}\")\n",
    "    model_parameters_amount = count_parameters(model)  # Assume this function is defined elsewhere\n",
    "    pprint(f\"Total parameters: {model_parameters_amount:,}\")\n",
    "\n",
    "    model = model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lr = 0.001\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    pprint(f\"Learning rate={lr}\")\n",
    "    epochs = 25\n",
    "\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        for phase in phases:\n",
    "            running_loss = 0.0\n",
    "            correct_predictions = [0, 0, 0, 0]  # Track correct predictions for each of the 4 targets\n",
    "            total_samples = 0\n",
    "            model.train() if phase == 'train' else model.eval()  # Simplified model mode setting\n",
    "\n",
    "            for inputs, labels in tqdm(loaders[phase]):  # Iterate over data.\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)  # [batch_size, 4, 19]\n",
    "                    if reshape:\n",
    "                        outputs = outputs.reshape(labels.shape[0], 4, -1)\n",
    "                    loss = sum([criterion(outputs[:, i, :], labels[:, i]) for i in range(4)])  # Sum loss across all targets\n",
    "\n",
    "                    if phase == 'train':  # backward + optimize only if in training phase\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                for i in range(4):\n",
    "                    _, predicted = torch.max(outputs[:, i, :], 1)\n",
    "                    correct_predictions[i] += (predicted == labels[:, i]).sum().item()\n",
    "\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "            avg_loss = running_loss / total_samples\n",
    "            top1_accuracy = [cp / total_samples * 100 for cp in correct_predictions]  # Accuracy per target\n",
    "            pprint(f\"Epoch [{epoch+1}/{epochs}], phase: {phase}, samples: {total_samples}, Loss: {avg_loss:.4f}, \"\n",
    "                  f\"Top-1 Accuracies: {[f'{acc:.2f}%' for acc in top1_accuracy]}\")\n",
    "\n",
    "    end = time.time()\n",
    "    pprint(f\"Elapsed time: {end - start} seconds\")\n",
    "\n",
    "    if save_weight:\n",
    "        torch.save(model.state_dict(), f'{model_name}.pt')  # It's often better to save state_dict\n",
    "        pprint(f\"Weight saved as: {model_name}.pt\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'denoise2_x'])\n",
      "origin_X shape: (5000, 50, 130)\n",
      "origin_Y shape: (5000, 4)\n",
      "origin_Y_onehot shape: (5000, 4, 19)\n",
      "(5000, 50, 130)\n",
      "5000\n",
      "--------- INDEX checking ---------\n",
      "Original: [0, 1, 2, 3, 4]\n",
      "Shuffled: [1784, 2015, 3988, 139, 639]\n",
      "--------- INDEX shuffled ---------\n",
      "\n",
      "Total number of samples: 5000 datapoints\n",
      "Number of train samples: 125 batches/ 4000 datapoints\n",
      "Number of val samples: 16 batches/ 500 datapoints\n",
      "Number of test samples: 16 batches/ 500 datapoints\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'y_onehot', 'x', 'y'])\n",
      "origin_X shape: (3000,)\n",
      "origin_Y shape: (3000, 4)\n",
      "origin_Y_onehot shape: (3000, 4, 19)\n",
      "(3000,)\n",
      "(3000, 50, 130, 3)\n",
      "3000\n",
      "Training model: SimpleCharCNN\n",
      "Total parameters: 4,845,196\n",
      "Learning rate=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 52.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], phase: train, samples: 4000, Loss: 0.3619, Top-1 Accuracies: ['11.15%', '12.85%', '12.45%', '10.78%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 123.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], phase: val, samples: 500, Loss: 0.3780, Top-1 Accuracies: ['9.40%', '5.80%', '7.80%', '4.80%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 57.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25], phase: train, samples: 4000, Loss: 0.3489, Top-1 Accuracies: ['22.53%', '23.30%', '24.70%', '22.55%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 145.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25], phase: val, samples: 500, Loss: 0.3599, Top-1 Accuracies: ['25.00%', '15.60%', '25.40%', '19.20%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 56.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25], phase: train, samples: 4000, Loss: 0.3336, Top-1 Accuracies: ['37.03%', '36.55%', '36.38%', '34.08%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 160.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25], phase: val, samples: 500, Loss: 0.3473, Top-1 Accuracies: ['32.00%', '30.40%', '38.80%', '25.00%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 57.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25], phase: train, samples: 4000, Loss: 0.3283, Top-1 Accuracies: ['40.85%', '40.88%', '40.77%', '38.77%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 160.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25], phase: val, samples: 500, Loss: 0.3318, Top-1 Accuracies: ['44.60%', '47.80%', '41.00%', '43.60%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 56.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/25], phase: train, samples: 4000, Loss: 0.3162, Top-1 Accuracies: ['52.20%', '51.08%', '49.28%', '48.05%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 160.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/25], phase: val, samples: 500, Loss: 0.3212, Top-1 Accuracies: ['56.00%', '54.00%', '48.40%', '48.20%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 57.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/25], phase: train, samples: 4000, Loss: 0.3064, Top-1 Accuracies: ['58.85%', '58.03%', '58.70%', '56.62%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 160.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/25], phase: val, samples: 500, Loss: 0.3117, Top-1 Accuracies: ['60.40%', '57.80%', '62.80%', '58.00%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 56.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/25], phase: train, samples: 4000, Loss: 0.3003, Top-1 Accuracies: ['61.68%', '62.70%', '65.18%', '62.50%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 159.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/25], phase: val, samples: 500, Loss: 0.3058, Top-1 Accuracies: ['65.00%', '67.60%', '66.00%', '58.40%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 57.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/25], phase: train, samples: 4000, Loss: 0.2963, Top-1 Accuracies: ['64.05%', '68.90%', '67.35%', '64.55%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 160.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/25], phase: val, samples: 500, Loss: 0.3011, Top-1 Accuracies: ['67.60%', '70.60%', '69.80%', '64.20%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 57.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/25], phase: train, samples: 4000, Loss: 0.2938, Top-1 Accuracies: ['65.38%', '70.45%', '69.38%', '67.17%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 160.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/25], phase: val, samples: 500, Loss: 0.3022, Top-1 Accuracies: ['68.20%', '67.80%', '68.40%', '63.80%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 57.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/25], phase: train, samples: 4000, Loss: 0.2920, Top-1 Accuracies: ['66.30%', '74.00%', '70.70%', '67.35%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 160.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/25], phase: val, samples: 500, Loss: 0.2997, Top-1 Accuracies: ['69.60%', '71.40%', '70.40%', '65.00%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 56.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/25], phase: train, samples: 4000, Loss: 0.2914, Top-1 Accuracies: ['66.47%', '74.15%', '72.17%', '67.00%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 159.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/25], phase: val, samples: 500, Loss: 0.2982, Top-1 Accuracies: ['68.00%', '75.20%', '72.80%', '64.60%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 56.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/25], phase: train, samples: 4000, Loss: 0.2886, Top-1 Accuracies: ['68.55%', '75.83%', '75.62%', '68.97%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 159.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/25], phase: val, samples: 500, Loss: 0.3025, Top-1 Accuracies: ['68.00%', '72.60%', '68.80%', '58.20%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 56.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/25], phase: train, samples: 4000, Loss: 0.2895, Top-1 Accuracies: ['67.65%', '75.02%', '74.98%', '68.05%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 160.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/25], phase: val, samples: 500, Loss: 0.2975, Top-1 Accuracies: ['69.60%', '73.40%', '75.80%', '63.60%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 56.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/25], phase: train, samples: 4000, Loss: 0.2875, Top-1 Accuracies: ['69.00%', '77.88%', '76.88%', '68.88%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 160.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/25], phase: val, samples: 500, Loss: 0.2950, Top-1 Accuracies: ['71.00%', '79.00%', '75.60%', '65.20%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 57.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/25], phase: train, samples: 4000, Loss: 0.2867, Top-1 Accuracies: ['70.23%', '78.60%', '77.58%', '68.60%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 160.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/25], phase: val, samples: 500, Loss: 0.2963, Top-1 Accuracies: ['70.20%', '76.40%', '75.20%', '64.60%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 57.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/25], phase: train, samples: 4000, Loss: 0.2870, Top-1 Accuracies: ['70.05%', '78.25%', '77.08%', '68.50%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 160.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/25], phase: val, samples: 500, Loss: 0.2963, Top-1 Accuracies: ['71.80%', '76.80%', '76.00%', '61.80%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 57.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/25], phase: train, samples: 4000, Loss: 0.2863, Top-1 Accuracies: ['70.75%', '78.50%', '77.58%', '69.27%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 160.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/25], phase: val, samples: 500, Loss: 0.2946, Top-1 Accuracies: ['71.40%', '78.40%', '77.00%', '65.60%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 57.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/25], phase: train, samples: 4000, Loss: 0.2860, Top-1 Accuracies: ['70.47%', '79.55%', '78.05%', '69.10%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 159.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/25], phase: val, samples: 500, Loss: 0.2954, Top-1 Accuracies: ['72.40%', '77.20%', '76.00%', '64.40%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 57.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/25], phase: train, samples: 4000, Loss: 0.2860, Top-1 Accuracies: ['70.38%', '79.27%', '78.40%', '68.95%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 160.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/25], phase: val, samples: 500, Loss: 0.2950, Top-1 Accuracies: ['72.00%', '76.80%', '77.20%', '64.80%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 57.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/25], phase: train, samples: 4000, Loss: 0.2858, Top-1 Accuracies: ['70.28%', '79.60%', '78.60%', '69.33%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 145.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/25], phase: val, samples: 500, Loss: 0.2956, Top-1 Accuracies: ['71.80%', '75.20%', '76.20%', '65.20%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 57.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/25], phase: train, samples: 4000, Loss: 0.2863, Top-1 Accuracies: ['70.33%', '78.57%', '78.30%', '69.00%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 159.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/25], phase: val, samples: 500, Loss: 0.2954, Top-1 Accuracies: ['71.80%', '76.60%', '77.80%', '63.40%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 56.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/25], phase: train, samples: 4000, Loss: 0.2861, Top-1 Accuracies: ['70.10%', '79.42%', '78.15%', '69.15%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 160.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/25], phase: val, samples: 500, Loss: 0.2945, Top-1 Accuracies: ['72.20%', '77.40%', '77.00%', '65.40%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 57.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/25], phase: train, samples: 4000, Loss: 0.2858, Top-1 Accuracies: ['70.50%', '79.77%', '78.15%', '69.25%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 159.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/25], phase: val, samples: 500, Loss: 0.2951, Top-1 Accuracies: ['73.60%', '77.20%', '75.20%', '64.20%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 57.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/25], phase: train, samples: 4000, Loss: 0.2850, Top-1 Accuracies: ['71.43%', '80.47%', '78.85%', '69.77%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 145.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/25], phase: val, samples: 500, Loss: 0.2941, Top-1 Accuracies: ['72.60%', '77.00%', '78.20%', '65.60%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 56.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/25], phase: train, samples: 4000, Loss: 0.2852, Top-1 Accuracies: ['71.83%', '79.33%', '78.80%', '69.73%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 160.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/25], phase: val, samples: 500, Loss: 0.2929, Top-1 Accuracies: ['73.40%', '79.80%', '78.80%', '65.40%']\n",
      "Elapsed time: 57.589746952056885 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleCharCNN(\n",
       "  (conv11_W1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
       "  (conv12_W1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (max_pool1_W1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv23_W1): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
       "  (conv24_W1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv25_W1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn1_W1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (max_pool2_W1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv36_W1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "  (conv37_W1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv38_W1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (max_pool3_W1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv49_W1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  (conv410_W1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (bn2_W1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (max_pool4_W1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc_branches): ModuleList(\n",
       "    (0-3): 4 x Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Linear(in_features=128, out_features=19, bias=True)\n",
       "      (6): Softmax(dim=1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = get_data('dn2')\n",
    "model_list ={\n",
    "    \"SimpleCharCNN\": lambda: SimpleCharCNN(),\n",
    "}\n",
    "model_name = \"SimpleCharCNN\"\n",
    "phases = ['train', 'val']\n",
    "loaders = get_dataloaders(train_dataset, 0.8, 0.5, 32)\n",
    "test_dataset = get_data(\"test\")\n",
    "\n",
    "train(model_list, model_name, loaders, phases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- INDEX checking ---------\n",
      "Original: [0, 1, 2, 3, 4]\n",
      "Shuffled: [3032, 3195, 1239, 2887, 2873]\n",
      "--------- INDEX shuffled ---------\n",
      "\n",
      "Total number of samples: 5000 datapoints\n",
      "Number of train samples: 125 batches/ 4000 datapoints\n",
      "Number of val samples: 16 batches/ 500 datapoints\n",
      "Number of test samples: 16 batches/ 500 datapoints\n",
      "\n",
      "Training model: r6_btnk\n",
      "Total parameters: 7,284\n",
      "Learning rate=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 86.05it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], phase: train, samples: 4000, Loss: 0.3679, Top-1 Accuracies: ['7.05%', '6.60%', '7.15%', '7.70%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 266.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], phase: val, samples: 500, Loss: 0.3727, Top-1 Accuracies: ['8.80%', '7.20%', '8.20%', '10.40%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 107.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25], phase: train, samples: 4000, Loss: 0.3597, Top-1 Accuracies: ['9.78%', '9.93%', '10.65%', '11.38%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 399.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25], phase: val, samples: 500, Loss: 0.3647, Top-1 Accuracies: ['10.00%', '10.60%', '10.60%', '15.80%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 119.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25], phase: train, samples: 4000, Loss: 0.3516, Top-1 Accuracies: ['11.18%', '13.05%', '12.88%', '16.53%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 400.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25], phase: val, samples: 500, Loss: 0.3573, Top-1 Accuracies: ['11.20%', '11.20%', '9.80%', '16.00%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 108.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25], phase: train, samples: 4000, Loss: 0.3426, Top-1 Accuracies: ['12.70%', '14.57%', '15.47%', '19.23%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 228.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25], phase: val, samples: 500, Loss: 0.3517, Top-1 Accuracies: ['12.00%', '14.00%', '13.60%', '15.60%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 105.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/25], phase: train, samples: 4000, Loss: 0.3334, Top-1 Accuracies: ['13.88%', '16.07%', '17.57%', '21.82%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 533.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/25], phase: val, samples: 500, Loss: 0.3418, Top-1 Accuracies: ['14.00%', '16.40%', '13.60%', '17.80%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 110.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/25], phase: train, samples: 4000, Loss: 0.3256, Top-1 Accuracies: ['15.25%', '18.15%', '18.65%', '23.50%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 320.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/25], phase: val, samples: 500, Loss: 0.3366, Top-1 Accuracies: ['15.80%', '17.40%', '15.00%', '19.80%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 105.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/25], phase: train, samples: 4000, Loss: 0.3189, Top-1 Accuracies: ['16.35%', '18.90%', '19.95%', '25.65%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 400.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/25], phase: val, samples: 500, Loss: 0.3344, Top-1 Accuracies: ['16.60%', '18.00%', '14.60%', '19.60%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 111.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/25], phase: train, samples: 4000, Loss: 0.3128, Top-1 Accuracies: ['17.45%', '19.57%', '20.55%', '27.32%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 320.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/25], phase: val, samples: 500, Loss: 0.3321, Top-1 Accuracies: ['17.80%', '17.00%', '15.20%', '23.40%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 105.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/25], phase: train, samples: 4000, Loss: 0.3074, Top-1 Accuracies: ['18.50%', '20.42%', '20.72%', '29.20%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 399.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/25], phase: val, samples: 500, Loss: 0.3270, Top-1 Accuracies: ['17.40%', '18.40%', '15.60%', '23.20%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 108.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/25], phase: train, samples: 4000, Loss: 0.3025, Top-1 Accuracies: ['19.35%', '20.95%', '21.75%', '31.45%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 533.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/25], phase: val, samples: 500, Loss: 0.3229, Top-1 Accuracies: ['16.80%', '18.40%', '14.40%', '26.20%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 111.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/25], phase: train, samples: 4000, Loss: 0.2979, Top-1 Accuracies: ['20.15%', '21.45%', '21.93%', '33.50%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 266.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/25], phase: val, samples: 500, Loss: 0.3173, Top-1 Accuracies: ['18.00%', '20.40%', '13.80%', '27.40%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 106.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/25], phase: train, samples: 4000, Loss: 0.2937, Top-1 Accuracies: ['20.62%', '22.02%', '22.27%', '35.70%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 400.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/25], phase: val, samples: 500, Loss: 0.3148, Top-1 Accuracies: ['17.40%', '21.40%', '15.20%', '31.00%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 111.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/25], phase: train, samples: 4000, Loss: 0.2896, Top-1 Accuracies: ['20.90%', '21.82%', '22.50%', '37.38%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 400.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/25], phase: val, samples: 500, Loss: 0.3083, Top-1 Accuracies: ['18.00%', '23.00%', '16.20%', '34.00%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 116.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/25], phase: train, samples: 4000, Loss: 0.2857, Top-1 Accuracies: ['21.52%', '21.70%', '22.68%', '39.20%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 400.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/25], phase: val, samples: 500, Loss: 0.3022, Top-1 Accuracies: ['18.60%', '21.40%', '16.80%', '34.80%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 120.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/25], phase: train, samples: 4000, Loss: 0.2818, Top-1 Accuracies: ['21.82%', '21.88%', '22.88%', '41.42%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 400.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/25], phase: val, samples: 500, Loss: 0.2993, Top-1 Accuracies: ['18.20%', '20.80%', '16.60%', '34.20%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 123.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/25], phase: train, samples: 4000, Loss: 0.2780, Top-1 Accuracies: ['22.48%', '22.15%', '22.93%', '43.77%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 400.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/25], phase: val, samples: 500, Loss: 0.2926, Top-1 Accuracies: ['20.00%', '21.40%', '17.80%', '38.80%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 128.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/25], phase: train, samples: 4000, Loss: 0.2744, Top-1 Accuracies: ['22.95%', '22.43%', '23.03%', '45.90%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 533.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/25], phase: val, samples: 500, Loss: 0.2902, Top-1 Accuracies: ['19.20%', '22.60%', '17.20%', '41.60%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 123.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/25], phase: train, samples: 4000, Loss: 0.2709, Top-1 Accuracies: ['23.60%', '23.05%', '23.60%', '48.08%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 228.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/25], phase: val, samples: 500, Loss: 0.2873, Top-1 Accuracies: ['21.40%', '21.40%', '17.60%', '43.20%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 120.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/25], phase: train, samples: 4000, Loss: 0.2675, Top-1 Accuracies: ['23.75%', '23.62%', '23.77%', '50.08%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 533.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/25], phase: val, samples: 500, Loss: 0.2803, Top-1 Accuracies: ['20.40%', '23.20%', '18.60%', '47.20%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 116.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/25], phase: train, samples: 4000, Loss: 0.2641, Top-1 Accuracies: ['24.00%', '23.60%', '23.35%', '51.92%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 400.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/25], phase: val, samples: 500, Loss: 0.2782, Top-1 Accuracies: ['22.40%', '22.80%', '19.40%', '48.20%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 105.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/25], phase: train, samples: 4000, Loss: 0.2609, Top-1 Accuracies: ['24.50%', '24.32%', '23.38%', '53.60%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 400.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/25], phase: val, samples: 500, Loss: 0.2768, Top-1 Accuracies: ['20.80%', '21.80%', '18.80%', '49.00%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 106.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/25], phase: train, samples: 4000, Loss: 0.2578, Top-1 Accuracies: ['25.27%', '24.35%', '23.67%', '54.85%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 533.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/25], phase: val, samples: 500, Loss: 0.2726, Top-1 Accuracies: ['21.60%', '21.80%', '19.00%', '50.40%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 104.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/25], phase: train, samples: 4000, Loss: 0.2548, Top-1 Accuracies: ['25.97%', '24.45%', '24.20%', '56.62%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 533.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/25], phase: val, samples: 500, Loss: 0.2679, Top-1 Accuracies: ['23.40%', '22.00%', '20.00%', '53.60%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 106.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/25], phase: train, samples: 4000, Loss: 0.2520, Top-1 Accuracies: ['26.62%', '24.52%', '24.40%', '58.38%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 320.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/25], phase: val, samples: 500, Loss: 0.2666, Top-1 Accuracies: ['23.80%', '23.20%', '19.00%', '55.40%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 104.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/25], phase: train, samples: 4000, Loss: 0.2493, Top-1 Accuracies: ['27.35%', '24.60%', '24.43%', '60.22%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 400.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/25], phase: val, samples: 500, Loss: 0.2656, Top-1 Accuracies: ['23.00%', '22.40%', '18.40%', '57.80%']\n",
      "Elapsed time: 29.52579379081726 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mod_resnet(\n",
       "  (conv1): Conv2d(3, 8, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(8, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=32, out_features=76, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_list ={\n",
    "    \"SimpleCharCNN\": lambda: SimpleCharCNN(),\n",
    "    \"resnet18\": lambda: models.resnet18(weights = models.ResNet18_Weights.DEFAULT),\n",
    "    \"resnet152\": lambda: models.resnet152(weights = models.ResNet152_Weights.DEFAULT),\n",
    "    # \"r6_btnk\": lambda: mod_resnet(Bottleneck, [2, 2, 0, 0], channel_num_list=[16, 16, 16], num_classes=76)\n",
    "    \"r6_btnk\": lambda: mod_resnet(Bottleneck, [2, 2, 0, 0], channel_num_list=[8, 4, 8], num_classes=76)\n",
    "}\n",
    "model_name = \"r6_btnk\"\n",
    "phases = ['train', 'val']\n",
    "loaders = get_dataloaders(train_dataset, 0.8, 0.5, 32)\n",
    "train(model_list, model_name, loaders, phases, reshape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "class mod_resnet(nn.Module):\n",
    "    def __init__(self, block, layers, channel_num_list, num_classes=1000):\n",
    "        super(mod_resnet, self).__init__()\n",
    "        self.in_channels = channel_num_list[0]\n",
    "        self.conv1 = nn.Conv2d(3, channel_num_list[0], kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channel_num_list[0])\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, channel_num_list[1], layers[0])\n",
    "        self.layer2 = self._make_layer(block, channel_num_list[2], layers[1], stride=2)\n",
    "        # self.layer3 = self._make_layer(block, channel_num_list[3], layers[2], stride=2)\n",
    "        # self.layer4 = self._make_layer(block, channel_num_list[4], layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(channel_num_list[-1] * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        # x = self.layer3(x)\n",
    "        # x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
